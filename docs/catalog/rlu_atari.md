<div itemscope itemtype="http://schema.org/Dataset">
  <div itemscope itemprop="includedInDataCatalog" itemtype="http://schema.org/DataCatalog">
    <meta itemprop="name" content="TensorFlow Datasets" />
  </div>
  <meta itemprop="name" content="rlu_atari" />
  <meta itemprop="description" content="RL Unplugged is suite of benchmarks for offline reinforcement learning. The RL&#10;Unplugged is designed around the following considerations: to facilitate ease of&#10;use, we provide the datasets with a unified API which makes it easy for the&#10;practitioner to work with all data in the suite once a general pipeline has been&#10;established.&#10;&#10;&#10;We are releasing a large and diverse dataset of gameplay following the protocol&#10;described by [Agarwal et al., 2020](https://arxiv.org/abs/1907.04543), which can&#10;be used to evaluate several discrete offline RL algorithms. The dataset is&#10;generated by running an online DQN agent and recording transitions from its&#10;replay during training with sticky actions&#10;[Machado et al., 2018](https://arxiv.org/abs/1709.06009). As stated in&#10;[Agarwal et al., 2020](https://arxiv.org/abs/1907.04543), for each game we use&#10;data from five runs with 50 million transitions each. States in each transition&#10;include stacks of four frames to be able to do frame-stacking with our&#10;baselines. We release datasets for 46 Atari games. For details on how the&#10;dataset was generated, please refer to the paper.&#10;&#10;Atari is a standard RL benchmark. We recommend you to try offline RL methods on&#10;Atari if you are interested in comparing your approach to other state of the art&#10;offline RL methods with discrete actions.&#10;&#10;Besides the reward of each step, this dataset includes the clipped reward&#10;(obtained with [-1, 1] clipping) and the sum of the clipped reward per episode.&#10;&#10;To use this dataset:&#10;&#10;```python&#10;import tensorflow_datasets as tfds&#10;&#10;ds = tfds.load(&#x27;rlu_atari&#x27;, split=&#x27;train&#x27;)&#10;for ex in ds.take(4):&#10;  print(ex)&#10;```&#10;&#10;See [the guide](https://www.tensorflow.org/datasets/overview) for more&#10;informations on [tensorflow_datasets](https://www.tensorflow.org/datasets).&#10;&#10;" />
  <meta itemprop="url" content="https://www.tensorflow.org/datasets/catalog/rlu_atari" />
  <meta itemprop="sameAs" content="https://github.com/deepmind/deepmind-research/tree/master/rl_unplugged" />
  <meta itemprop="citation" content="@misc{gulcehre2020rl,&#10;    title={RL Unplugged: Benchmarks for Offline Reinforcement Learning},&#10;    author={Caglar Gulcehre and Ziyu Wang and Alexander Novikov and Tom Le Paine&#10;        and  Sergio Gómez Colmenarejo and Konrad Zolna and Rishabh Agarwal and&#10;        Josh Merel and Daniel Mankowitz and Cosmin Paduraru and Gabriel&#10;        Dulac-Arnold and Jerry Li and Mohammad Norouzi and Matt Hoffman and&#10;        Ofir Nachum and George Tucker and Nicolas Heess and Nando deFreitas},&#10;    year={2020},&#10;    eprint={2006.13888},&#10;    archivePrefix={arXiv},&#10;    primaryClass={cs.LG}&#10;}" />
</div>

# `rlu_atari`


Note: This dataset was added recently and is only available in our
`tfds-nightly` package
<span class="material-icons" title="Available only in the tfds-nightly package">nights_stay</span>.

*   **Description**:

RL Unplugged is suite of benchmarks for offline reinforcement learning. The RL
Unplugged is designed around the following considerations: to facilitate ease of
use, we provide the datasets with a unified API which makes it easy for the
practitioner to work with all data in the suite once a general pipeline has been
established.

We are releasing a large and diverse dataset of gameplay following the protocol
described by [Agarwal et al., 2020](https://arxiv.org/abs/1907.04543), which can
be used to evaluate several discrete offline RL algorithms. The dataset is
generated by running an online DQN agent and recording transitions from its
replay during training with sticky actions
[Machado et al., 2018](https://arxiv.org/abs/1709.06009). As stated in
[Agarwal et al., 2020](https://arxiv.org/abs/1907.04543), for each game we use
data from five runs with 50 million transitions each. States in each transition
include stacks of four frames to be able to do frame-stacking with our
baselines. We release datasets for 46 Atari games. For details on how the
dataset was generated, please refer to the paper.

Atari is a standard RL benchmark. We recommend you to try offline RL methods on
Atari if you are interested in comparing your approach to other state of the art
offline RL methods with discrete actions.

Besides the reward of each step, this dataset includes the clipped reward
(obtained with [-1, 1] clipping) and the sum of the clipped reward per episode.

*   **Homepage**:
    [https://github.com/deepmind/deepmind-research/tree/master/rl_unplugged](https://github.com/deepmind/deepmind-research/tree/master/rl_unplugged)

*   **Source code**:
    [`tfds.rl_unplugged.rlu_atari.RluAtari`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/rl_unplugged/rlu_atari/rlu_atari.py)

*   **Versions**:

    *   **`1.0.0`** (default): Initial release.

*   **Download size**: `Unknown size`

*   **Dataset size**: `Unknown size`

*   **Auto-cached**
    ([documentation](https://www.tensorflow.org/datasets/performances#auto-caching)):
    Unknown

*   **Splits**:

Split | Examples
:---- | -------:

*   **Features**:

```python
FeaturesDict({
    'clipped_episode_return': tf.float32,
    'episode_id': tf.int64,
    'episode_return': tf.float32,
    'steps': Dataset({
        'action': tf.int64,
        'clipped_reward': tf.float32,
        'discount': tf.float32,
        'is_first': tf.bool,
        'is_terminal': tf.bool,
        'observation': Image(shape=(84, 84, 1), dtype=tf.uint8),
        'reward': tf.float32,
    }),
})
```

*   **Supervised keys** (See
    [`as_supervised` doc](https://www.tensorflow.org/datasets/api_docs/python/tfds/load#args)):
    `None`

*   **Figure**
    ([tfds.show_examples](https://www.tensorflow.org/datasets/api_docs/python/tfds/visualization/show_examples)):
    Not supported.

*   **Examples**
    ([tfds.as_dataframe](https://www.tensorflow.org/datasets/api_docs/python/tfds/as_dataframe)):
    Missing.

*   **Citation**:

```
@misc{gulcehre2020rl,
    title={RL Unplugged: Benchmarks for Offline Reinforcement Learning},
    author={Caglar Gulcehre and Ziyu Wang and Alexander Novikov and Tom Le Paine
        and  Sergio Gómez Colmenarejo and Konrad Zolna and Rishabh Agarwal and
        Josh Merel and Daniel Mankowitz and Cosmin Paduraru and Gabriel
        Dulac-Arnold and Jerry Li and Mohammad Norouzi and Matt Hoffman and
        Ofir Nachum and George Tucker and Nicolas Heess and Nando deFreitas},
    year={2020},
    eprint={2006.13888},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
```

## rlu_atari/Alien_run_1 (default config)

## rlu_atari/Alien_run_2

## rlu_atari/Alien_run_3

## rlu_atari/Alien_run_4

## rlu_atari/Alien_run_5

## rlu_atari/Amidar_run_1

## rlu_atari/Amidar_run_2

## rlu_atari/Amidar_run_3

## rlu_atari/Amidar_run_4

## rlu_atari/Amidar_run_5

## rlu_atari/Assault_run_1

## rlu_atari/Assault_run_2

## rlu_atari/Assault_run_3

## rlu_atari/Assault_run_4

## rlu_atari/Assault_run_5

## rlu_atari/Asterix_run_1

## rlu_atari/Asterix_run_2

## rlu_atari/Asterix_run_3

## rlu_atari/Asterix_run_4

## rlu_atari/Asterix_run_5

## rlu_atari/Atlantis_run_1

## rlu_atari/Atlantis_run_2

## rlu_atari/Atlantis_run_3

## rlu_atari/Atlantis_run_4

## rlu_atari/Atlantis_run_5

## rlu_atari/BankHeist_run_1

## rlu_atari/BankHeist_run_2

## rlu_atari/BankHeist_run_3

## rlu_atari/BankHeist_run_4

## rlu_atari/BankHeist_run_5

## rlu_atari/BattleZone_run_1

## rlu_atari/BattleZone_run_2

## rlu_atari/BattleZone_run_3

## rlu_atari/BattleZone_run_4

## rlu_atari/BattleZone_run_5

## rlu_atari/BeamRider_run_1

## rlu_atari/BeamRider_run_2

## rlu_atari/BeamRider_run_3

## rlu_atari/BeamRider_run_4

## rlu_atari/BeamRider_run_5

## rlu_atari/Boxing_run_1

## rlu_atari/Boxing_run_2

## rlu_atari/Boxing_run_3

## rlu_atari/Boxing_run_4

## rlu_atari/Boxing_run_5

## rlu_atari/Breakout_run_1

## rlu_atari/Breakout_run_2

## rlu_atari/Breakout_run_3

## rlu_atari/Breakout_run_4

## rlu_atari/Breakout_run_5

## rlu_atari/Carnival_run_1

## rlu_atari/Carnival_run_2

## rlu_atari/Carnival_run_3

## rlu_atari/Carnival_run_4

## rlu_atari/Carnival_run_5

## rlu_atari/Centipede_run_1

## rlu_atari/Centipede_run_2

## rlu_atari/Centipede_run_3

## rlu_atari/Centipede_run_4

## rlu_atari/Centipede_run_5

## rlu_atari/ChopperCommand_run_1

## rlu_atari/ChopperCommand_run_2

## rlu_atari/ChopperCommand_run_3

## rlu_atari/ChopperCommand_run_4

## rlu_atari/ChopperCommand_run_5

## rlu_atari/CrazyClimber_run_1

## rlu_atari/CrazyClimber_run_2

## rlu_atari/CrazyClimber_run_3

## rlu_atari/CrazyClimber_run_4

## rlu_atari/CrazyClimber_run_5

## rlu_atari/DemonAttack_run_1

## rlu_atari/DemonAttack_run_2

## rlu_atari/DemonAttack_run_3

## rlu_atari/DemonAttack_run_4

## rlu_atari/DemonAttack_run_5

## rlu_atari/DoubleDunk_run_1

## rlu_atari/DoubleDunk_run_2

## rlu_atari/DoubleDunk_run_3

## rlu_atari/DoubleDunk_run_4

## rlu_atari/DoubleDunk_run_5

## rlu_atari/Enduro_run_1

## rlu_atari/Enduro_run_2

## rlu_atari/Enduro_run_3

## rlu_atari/Enduro_run_4

## rlu_atari/Enduro_run_5

## rlu_atari/FishingDerby_run_1

## rlu_atari/FishingDerby_run_2

## rlu_atari/FishingDerby_run_3

## rlu_atari/FishingDerby_run_4

## rlu_atari/FishingDerby_run_5

## rlu_atari/Freeway_run_1

## rlu_atari/Freeway_run_2

## rlu_atari/Freeway_run_3

## rlu_atari/Freeway_run_4

## rlu_atari/Freeway_run_5

## rlu_atari/Frostbite_run_1

## rlu_atari/Frostbite_run_2

## rlu_atari/Frostbite_run_3

## rlu_atari/Frostbite_run_4

## rlu_atari/Frostbite_run_5

## rlu_atari/Gopher_run_1

## rlu_atari/Gopher_run_2

## rlu_atari/Gopher_run_3

## rlu_atari/Gopher_run_4

## rlu_atari/Gopher_run_5

## rlu_atari/Gravitar_run_1

## rlu_atari/Gravitar_run_2

## rlu_atari/Gravitar_run_3

## rlu_atari/Gravitar_run_4

## rlu_atari/Gravitar_run_5

## rlu_atari/Hero_run_1

## rlu_atari/Hero_run_2

## rlu_atari/Hero_run_3

## rlu_atari/Hero_run_4

## rlu_atari/Hero_run_5

## rlu_atari/IceHockey_run_1

## rlu_atari/IceHockey_run_2

## rlu_atari/IceHockey_run_3

## rlu_atari/IceHockey_run_4

## rlu_atari/IceHockey_run_5

## rlu_atari/Jamesbond_run_1

## rlu_atari/Jamesbond_run_2

## rlu_atari/Jamesbond_run_3

## rlu_atari/Jamesbond_run_4

## rlu_atari/Jamesbond_run_5

## rlu_atari/Kangaroo_run_1

## rlu_atari/Kangaroo_run_2

## rlu_atari/Kangaroo_run_3

## rlu_atari/Kangaroo_run_4

## rlu_atari/Kangaroo_run_5

## rlu_atari/Krull_run_1

## rlu_atari/Krull_run_2

## rlu_atari/Krull_run_3

## rlu_atari/Krull_run_4

## rlu_atari/Krull_run_5

## rlu_atari/KungFuMaster_run_1

## rlu_atari/KungFuMaster_run_2

## rlu_atari/KungFuMaster_run_3

## rlu_atari/KungFuMaster_run_4

## rlu_atari/KungFuMaster_run_5

## rlu_atari/MsPacman_run_1

## rlu_atari/MsPacman_run_2

## rlu_atari/MsPacman_run_3

## rlu_atari/MsPacman_run_4

## rlu_atari/MsPacman_run_5

## rlu_atari/NameThisGame_run_1

## rlu_atari/NameThisGame_run_2

## rlu_atari/NameThisGame_run_3

## rlu_atari/NameThisGame_run_4

## rlu_atari/NameThisGame_run_5

## rlu_atari/Phoenix_run_1

## rlu_atari/Phoenix_run_2

## rlu_atari/Phoenix_run_3

## rlu_atari/Phoenix_run_4

## rlu_atari/Phoenix_run_5

## rlu_atari/Pong_run_1

## rlu_atari/Pong_run_2

## rlu_atari/Pong_run_3

## rlu_atari/Pong_run_4

## rlu_atari/Pong_run_5

## rlu_atari/Pooyan_run_1

## rlu_atari/Pooyan_run_2

## rlu_atari/Pooyan_run_3

## rlu_atari/Pooyan_run_4

## rlu_atari/Pooyan_run_5

## rlu_atari/Qbert_run_1

## rlu_atari/Qbert_run_2

## rlu_atari/Qbert_run_3

## rlu_atari/Qbert_run_4

## rlu_atari/Qbert_run_5

## rlu_atari/Riverraid_run_1

## rlu_atari/Riverraid_run_2

## rlu_atari/Riverraid_run_3

## rlu_atari/Riverraid_run_4

## rlu_atari/Riverraid_run_5

## rlu_atari/RoadRunner_run_1

## rlu_atari/RoadRunner_run_2

## rlu_atari/RoadRunner_run_3

## rlu_atari/RoadRunner_run_4

## rlu_atari/RoadRunner_run_5

## rlu_atari/Robotank_run_1

## rlu_atari/Robotank_run_2

## rlu_atari/Robotank_run_3

## rlu_atari/Robotank_run_4

## rlu_atari/Robotank_run_5

## rlu_atari/Seaquest_run_1

## rlu_atari/Seaquest_run_2

## rlu_atari/Seaquest_run_3

## rlu_atari/Seaquest_run_4

## rlu_atari/Seaquest_run_5

## rlu_atari/SpaceInvaders_run_1

## rlu_atari/SpaceInvaders_run_2

## rlu_atari/SpaceInvaders_run_3

## rlu_atari/SpaceInvaders_run_4

## rlu_atari/SpaceInvaders_run_5

## rlu_atari/StarGunner_run_1

## rlu_atari/StarGunner_run_2

## rlu_atari/StarGunner_run_3

## rlu_atari/StarGunner_run_4

## rlu_atari/StarGunner_run_5

## rlu_atari/TimePilot_run_1

## rlu_atari/TimePilot_run_2

## rlu_atari/TimePilot_run_3

## rlu_atari/TimePilot_run_4

## rlu_atari/TimePilot_run_5

## rlu_atari/UpNDown_run_1

## rlu_atari/UpNDown_run_2

## rlu_atari/UpNDown_run_3

## rlu_atari/UpNDown_run_4

## rlu_atari/UpNDown_run_5

## rlu_atari/VideoPinball_run_1

## rlu_atari/VideoPinball_run_2

## rlu_atari/VideoPinball_run_3

## rlu_atari/VideoPinball_run_4

## rlu_atari/VideoPinball_run_5

## rlu_atari/WizardOfWor_run_1

## rlu_atari/WizardOfWor_run_2

## rlu_atari/WizardOfWor_run_3

## rlu_atari/WizardOfWor_run_4

## rlu_atari/WizardOfWor_run_5

## rlu_atari/YarsRevenge_run_1

## rlu_atari/YarsRevenge_run_2

## rlu_atari/YarsRevenge_run_3

## rlu_atari/YarsRevenge_run_4

## rlu_atari/YarsRevenge_run_5

## rlu_atari/Zaxxon_run_1

## rlu_atari/Zaxxon_run_2

## rlu_atari/Zaxxon_run_3

## rlu_atari/Zaxxon_run_4

## rlu_atari/Zaxxon_run_5
